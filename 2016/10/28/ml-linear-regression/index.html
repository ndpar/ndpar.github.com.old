
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Machine Learning: Linear Regression - Side Notes</title>
  <meta name="author" content="Andrey Paramonov">

  
  <meta name="description" content="Let $y$ be a dependent variable of a feature vector $x$ Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
find the &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.ndpar.com/2016/10/28/ml-linear-regression">
  <link href="/favicon.ico" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Side Notes" type="application/atom+xml">

  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>

  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Side Notes</a></h1>
  
    <h2>never finished…</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.ndpar.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Machine Learning: Linear Regression</h1>
    
    
      <p class="meta">
        








  



<time datetime="2016-10-28T17:05:31-04:00" pubdate data-updated="true">October 28, 2016</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>Let $y$ be a dependent variable of a feature vector $x$</p>

<script type="math/tex; mode=display">x =
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}</script>

<p>Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
find the value of $y$ on any input vector $x$.</p>

<p>We solve this problem by constructing a <em>hypothesis funciton</em> $h_\theta(x)$
using one of the methods below.</p>

<!-- more -->

<h2 id="notation">Notation</h2>

<script type="math/tex; mode=display">% <![CDATA[
x =
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
=
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
\quad
X =
\begin{pmatrix}
1 & x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_n \\
1 & x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_n \\
1 & x^{(3)}_1 & x^{(3)}_2 & \cdots & x^{(3)}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_n \\
\end{pmatrix}
\quad
y =
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)} \\
\end{pmatrix} %]]></script>

<h2 id="optimization-objective">Optimization Objective</h2>

<p>Step 1. Normalize each feature $(x^{(0)}_j, …, x^{(m)}_j)$, $1 \le j \le n$ by mean $\mu$ and standard deviation $\sigma$</p>

<script type="math/tex; mode=display">x^{(i)}_j \leftarrow \frac{x^{(i)}_j - \mu_j}{\sigma_j}</script>

<figure class="code"><figcaption><span>featureNormalize.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="matlab"><span class="line"><span class="k">function</span><span class="w"> </span>[X_norm, mu, sigma] <span class="p">=</span><span class="w"> </span><span class="nf">featureNormalize</span><span class="p">(</span>X<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="w">    </span><span class="n">mu</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">    <span class="n">sigma</span> <span class="p">=</span> <span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">    <span class="n">X_norm</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">./</span> <span class="n">sigma</span><span class="p">;</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>Step 2. Minimize the cost function</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \|X\theta - y\|^2 = \frac{1}{2m}(X\theta - y)^T (X\theta - y)</script>

<p>where $\theta = (\theta_0, …, \theta_n)^T$</p>

<p>Step 3. Compute hypothesis function as</p>

<script type="math/tex; mode=display">h_\theta(x) = x \cdot \theta = x^T \theta</script>

<p>where vector $x$ is normalized using the same values of $\mu$ and $\sigma$ as in Step 1.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient descent</em> is the method for finding (global) minimum of cost funtion $J(\theta)$.
There are few ways to implement this method.</p>

<h3 id="direct-method">Direct method</h3>

<p>Choose small learning rate $\alpha &gt; 0$ and find the fixed point of the function</p>

<script type="math/tex; mode=display">f(\theta) = \theta - \frac{\alpha}{m}X^T (X\theta - y)</script>

<figure class="code"><figcaption><span>gradientDescent.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="matlab"><span class="line"><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescent</span><span class="p">(</span>X, y, theta, alpha, num_iters<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="w">    </span><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</span><span class="line">    <span class="k">for</span> <span class="n">iter</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_iters</span>
</span><span class="line">        <span class="n">h</span> <span class="p">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</span><span class="line">        <span class="n">delta</span> <span class="p">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</span><span class="line">        <span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class="line">    <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<h3 id="optimized-method">Optimized method</h3>

<p>Many mathematical software packages already include implementations of gradient
descent that compute learning rate $\alpha$ automatically.
These methods accept cost function $J(\theta)$ and its gradient
$\nabla J(\theta)$ as arguments, which for the linear regression is computed as follows</p>

<script type="math/tex; mode=display">\nabla J(\theta) = \frac{1}{m}X^T (X\theta - y)</script>

<figure class="code"><figcaption><span>costFunction.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="matlab"><span class="line"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunction</span><span class="p">(</span>theta, X, y<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="w">    </span><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</span><span class="line">    <span class="n">h</span> <span class="p">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</span><span class="line">    <span class="n">delta</span> <span class="p">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</span><span class="line">    <span class="n">J</span> <span class="p">=</span> <span class="n">delta</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class="line">    <span class="n">grad</span> <span class="p">=</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<figure class="code"><figcaption><span>gradientDescent.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="matlab"><span class="line"><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescent</span><span class="p">(</span>X, y, initial_theta<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="w">    </span><span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
</span><span class="line">    <span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">]</span> <span class="p">=</span> <span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunction</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="normal-equation">Normal Equation</h2>

<p>Unlike Gradient Descent this method does not require feature normalization (Step 1) and convergence loop. <em>Normal equation</em> gives the closed-form solution to linear regression</p>

<script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y</script>

<figure class="code"><figcaption><span>normalEquation.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="matlab"><span class="line"><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">normalEquation</span><span class="p">(</span>X, y<span class="p">)</span><span class="w"></span>
</span><span class="line"><span class="w">    </span><span class="n">theta</span> <span class="p">=</span> <span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="regularization">Regularization</h2>

<p>In case of overfitting both methods can be tweaked by introducing polynomial
features and adjusting equations as follows. Let $\lambda &gt; 0$ and E be the
diagonal matrix</p>

<script type="math/tex; mode=display">% <![CDATA[
E =
\begin{pmatrix}
0 & & & & \\
& 1 & & & \\
& & 1 & & \\
& & & \ddots & \\
& & & & 1 \\
\end{pmatrix}_{n+1 \times n+1} %]]></script>

<p>Then the cost function for gradient descent becomes</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \left( \|h_\theta(X) - y\|^2 + \lambda \| E\theta \|^2 \right) \\
\nabla J(\theta) = \frac{1}{m} \left( X^T (h_\theta(X) - y) + \lambda E\theta \right)</script>

<p>and normal equation</p>

<script type="math/tex; mode=display">\theta = \left( X^T X +\lambda E \right)^{-1} X^T y</script>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Andrey Paramonov</span></span>

      








  



<time datetime="2016-10-28T17:05:31-04:00" pubdate data-updated="true">October 28, 2016</time>
      

<span class="categories">
  
    <a class='category' href='/categories/machine-learning/'>machine learning</a>, <a class='category' href='/categories/math/'>math</a>, <a class='category' href='/categories/matlab/'>matlab</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2015/04/03/feynman-tot/" title="Previous Post: Another Notation as a Tool of Thought">&laquo; Another Notation as a Tool of Thought</a>
      
      
        <a class="basic-alignment right" href="/2016/11/07/ml-logistic-regression/" title="Next Post: Machine Learning: Logistic Regression">Machine Learning: Logistic Regression &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2016/12/24/spring-oauth2/">Spring OAuth 2</a>
      </li>
    
      <li class="post">
        <a href="/2016/11/07/ml-logistic-regression/">Machine Learning: Logistic Regression</a>
      </li>
    
      <li class="post">
        <a href="/2016/10/28/ml-linear-regression/">Machine Learning: Linear Regression</a>
      </li>
    
      <li class="post">
        <a href="/2015/04/03/feynman-tot/">Another Notation as a Tool of Thought</a>
      </li>
    
      <li class="post">
        <a href="/2014/11/29/two-series/">Two Series</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Recent GitHub Updates</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ndpar',
            count: 4,
            skip_forks: false,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>




<section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/categories/activemq/'>activemq (1)</a></li>
<li class='category'><a href='/categories/agile/'>agile (1)</a></li>
<li class='category'><a href='/categories/art/'>art (1)</a></li>
<li class='category'><a href='/categories/book-review/'>book review (1)</a></li>
<li class='category'><a href='/categories/buzz/'>buzz (2)</a></li>
<li class='category'><a href='/categories/clojure/'>clojure (4)</a></li>
<li class='category'><a href='/categories/clustering/'>clustering (1)</a></li>
<li class='category'><a href='/categories/conference/'>conference (3)</a></li>
<li class='category'><a href='/categories/craftsmanship/'>craftsmanship (1)</a></li>
<li class='category'><a href='/categories/education/'>education (1)</a></li>
<li class='category'><a href='/categories/ejabberd/'>ejabberd (2)</a></li>
<li class='category'><a href='/categories/entrepreneurship/'>entrepreneurship (1)</a></li>
<li class='category'><a href='/categories/erlang/'>erlang (8)</a></li>
<li class='category'><a href='/categories/feynman/'>feynman (1)</a></li>
<li class='category'><a href='/categories/functional-programming/'>functional programming (1)</a></li>
<li class='category'><a href='/categories/git/'>git (3)</a></li>
<li class='category'><a href='/categories/grails/'>grails (1)</a></li>
<li class='category'><a href='/categories/groovy/'>groovy (10)</a></li>
<li class='category'><a href='/categories/haskell/'>haskell (1)</a></li>
<li class='category'><a href='/categories/humor/'>humor (2)</a></li>
<li class='category'><a href='/categories/j/'>j (2)</a></li>
<li class='category'><a href='/categories/java/'>java (4)</a></li>
<li class='category'><a href='/categories/lisp/'>lisp (3)</a></li>
<li class='category'><a href='/categories/machine-learning/'>machine learning (2)</a></li>
<li class='category'><a href='/categories/math/'>math (11)</a></li>
<li class='category'><a href='/categories/matlab/'>matlab (2)</a></li>
<li class='category'><a href='/categories/maven/'>maven (1)</a></li>
<li class='category'><a href='/categories/mongodb/'>mongodb (1)</a></li>
<li class='category'><a href='/categories/nosql/'>nosql (1)</a></li>
<li class='category'><a href='/categories/oauth/'>oauth (1)</a></li>
<li class='category'><a href='/categories/programming/'>programming (6)</a></li>
<li class='category'><a href='/categories/python/'>python (2)</a></li>
<li class='category'><a href='/categories/rabbitmq/'>rabbitmq (4)</a></li>
<li class='category'><a href='/categories/regex/'>regex (2)</a></li>
<li class='category'><a href='/categories/scala/'>scala (1)</a></li>
<li class='category'><a href='/categories/sicp/'>sicp (1)</a></li>
<li class='category'><a href='/categories/solr/'>solr (1)</a></li>
<li class='category'><a href='/categories/spring/'>spring (2)</a></li>
<li class='category'><a href='/categories/tutorial/'>tutorial (1)</a></li>
<li class='category'><a href='/categories/zeromq/'>zeromq (1)</a></li>
<li class='category'><a href='/categories/zookeeper/'>zookeeper (1)</a></li>

  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2009–2017 - Andrey Paramonov -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span> -
  <span class="credit">Hosted by <a href="http://github.com">GitHub</a></span>
</p>

</footer>
  











</body>
</html>
