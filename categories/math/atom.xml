<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: math | Side Notes]]></title>
  <link href="http://blog.ndpar.com/categories/math/atom.xml" rel="self"/>
  <link href="http://blog.ndpar.com/"/>
  <updated>2017-01-08T17:14:02-05:00</updated>
  <id>http://blog.ndpar.com/</id>
  <author>
    <name><![CDATA[Andrey Paramonov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning: Logistic Regression]]></title>
    <link href="http://blog.ndpar.com/2016/11/07/ml-logistic-regression/"/>
    <updated>2016-11-07T16:05:31-05:00</updated>
    <id>http://blog.ndpar.com/2016/11/07/ml-logistic-regression</id>
    <content type="html"><![CDATA[<p><em>Logistic regression</em> is a classification case of <a href="/2016/10/28/ml-linear-regression">linear regression</a> whith
dependent variable $y$ taking binary values.</p>

<p>Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
$x \in \mathbb{R}^{n+1}$, $x^{(i)} _ 0 = 0$, $y^{(i)} \in $ {0,1},
find <em>classification function</em></p>

<script type="math/tex; mode=display">h_\theta(x) = P(y = 1 | x; \theta)</script>

<!-- more -->

<h2 id="gradient-descent">Gradient Descent</h2>

<p>Let’s build function $h_\theta(x)$ as a sigmoid function of $\theta\cdot x$</p>

<script type="math/tex; mode=display">h_\theta(x) = g(\theta\cdot x), \quad
g(z) = \mathbb{sigmoid}(z) = \frac{1}{1 + e^{-z}}</script>

<p>Sigmoid function has rank infinity, i.e. it operates on scalars, vectors and matrices.</p>

<p><figure class='code'><figcaption><span>sigmoid.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">g</span> <span class="p">=</span> <span class="mi">1</span> <span class="o">./</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>To find optimal parameter $\theta \in \mathbb{R}^{n+1}$ we are going to use
optimized gradient descent method which takes as arguments
cost function $J(\theta)$ and its gradient.
For logistic regression they are</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m} \left( y^T \ln h_\theta(X) + (1-y)^T \ln (1-h_\theta(X)) \right) \\
\nabla J(\theta) = \frac{1}{m} X^T (h_\theta(X) - y)</script>

<p>where $X = (x^{(i)}_j) _{m \times n+1}$ is a matrix of the training examples from
the <a href="/2016/10/28/ml-linear-regression">previous lecture</a>.</p>

<p>Analogous to linear regression, logistic regression can be regularized too</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m} \left( y^T \ln h_\theta(X) + (1-y)^T \ln (1-h_\theta(X)) \right) + \frac{\lambda}{2m} \| E\theta \|^2 \\
\nabla J(\theta) = \frac{1}{m} \left( X^T (h_\theta(X) - y) + \lambda E\theta \right)</script>

<p><figure class='code'><figcaption><span>costFunction.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunction</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>
</span><span class='line'>    <span class="n">h</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
</span><span class='line'>    <span class="n">J</span> <span class="p">=</span> <span class="p">(</span><span class="n">y</span>’ <span class="o">*</span> <span class="nb">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>’ <span class="o">*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="o">-</span><span class="n">m</span><span class="p">;</span>
</span><span class='line'>    <span class="n">grad</span> <span class="p">=</span> <span class="n">X</span>’ <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="c">% Regularization</span>
</span><span class='line'><span class="n">th</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span> <span class="n">th</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="n">J</span> <span class="p">=</span> <span class="n">J</span> <span class="o">+</span> <span class="n">th</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">th</span> <span class="o">*</span> <span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'><span class="n">grad</span> <span class="p">=</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">th</span> <span class="o">*</span> <span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span> <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p>Having computed $\theta$ we can now implement the prediction function</p>

<p><figure class='code'><figcaption><span>predict.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>p <span class="p">=</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>theta, X<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">p</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;=</span> <span class="mf">0.5</span><span class="p">;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>which can be used to classify new examples and check the prediction accuracy
on the training set</p>

<p><figure class='code'><figcaption><span>accuracy.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>a <span class="p">=</span><span class="w"> </span><span class="nf">accuracy</span><span class="p">(</span>theta, X, y<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">p</span> <span class="p">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">);</span>
</span><span class='line'>    <span class="n">a</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">double</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<h2 id="multi-class-classification">Multi-class Classification</h2>

<p>Logistic regression works for binary $y$.
Suppose now that $y^{(i)} \in ${$1,…,K$}, where $K &gt; 2$.
In this case we can use <em>One-vs-All</em> variation of this algorithm.</p>

<p>Step 1. Convert vector $y$ into a binary matrix $Y$</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
y =
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)} \\
\end{pmatrix}
\quad
\rightarrow
\quad
Y =
\begin{pmatrix}
y^{(1)}_1 & y^{(1)}_2 & \cdots & y^{(1)}_K \\
y^{(2)}_1 & y^{(2)}_2 & \cdots & y^{(2)}_K \\
y^{(3)}_1 & y^{(3)}_2 & \cdots & y^{(3)}_K \\
\vdots & \vdots & \ddots & \vdots \\
y^{(m)}_1 & y^{(m)}_2 & \cdots & y^{(m)}_K \\
\end{pmatrix} %]]&gt;</script>

<p>where $y^{(i)}_k = \delta _{k y^{(i)}}$, i.e. $y^{(i)}_k = 1$ when $y^{(i)} = k$, otherwise $y^{(i)}_k = 0$.</p>

<p>Step 2. Train logistic classifier on every column of matrix $Y$.
The result will be a matrix $\Theta = (\theta_{jk})_{n+1 \times K}$</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\Theta =
\begin{pmatrix}
\theta_{01} & \theta_{02} & \cdots & \theta_{0K} \\
\theta_{11} & \theta_{12} & \cdots & \theta_{1K} \\
\theta_{21} & \theta_{22} & \cdots & \theta_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{n1} & \theta_{n2} & \cdots & \theta_{nK} \\
\end{pmatrix} %]]&gt;</script>

<p>Step 3. For any given vector $x$ compute vector $h = x^T \Theta$. Then
the predicted value $y$ will be</p>

<script type="math/tex; mode=display">y = \{ p \: | \: h_p = \mathbb{max} (h_k), 1 \le k \le K \}</script>

<p>To compute accuracy of the one-vs-all classifier on the training set
use <code>accuracy.m</code> script from above with modified <code>predict.m</code></p>

<p><figure class='code'><figcaption><span>predict.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>p <span class="p">=</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>Theta, X<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">a</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">Theta</span><span class="p">);</span>
</span><span class='line'>    <span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="p">=</span> <span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[],</span> <span class="mi">2</span><span class="p">);</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning: Linear Regression]]></title>
    <link href="http://blog.ndpar.com/2016/10/28/ml-linear-regression/"/>
    <updated>2016-10-28T17:05:31-04:00</updated>
    <id>http://blog.ndpar.com/2016/10/28/ml-linear-regression</id>
    <content type="html"><![CDATA[<p>Let $y$ be a dependent variable of a feature vector $x$</p>

<script type="math/tex; mode=display">x =
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}</script>

<p>Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
find the value of $y$ on any input vector $x$.</p>

<p>We solve this problem by constructing a <em>hypothesis funciton</em> $h_\theta(x)$
using one of the methods below.</p>

<!-- more -->

<h2 id="notation">Notation</h2>

<script type="math/tex; mode=display">% &lt;![CDATA[
x =
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
=
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
\quad
X =
\begin{pmatrix}
1 & x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_n \\
1 & x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_n \\
1 & x^{(3)}_1 & x^{(3)}_2 & \cdots & x^{(3)}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_n \\
\end{pmatrix}
\quad
y =
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)} \\
\end{pmatrix} %]]&gt;</script>

<h2 id="optimization-objective">Optimization Objective</h2>

<p>Step 1. Normalize each feature $(x^{(0)}_j, …, x^{(m)}_j)$, $1 \le j \le n$ by mean $\mu$ and standard deviation $\sigma$</p>

<script type="math/tex; mode=display">x^{(i)}_j \leftarrow \frac{x^{(i)}_j - \mu_j}{\sigma_j}</script>

<p><figure class='code'><figcaption><span>featureNormalize.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[X_norm, mu, sigma] <span class="p">=</span><span class="w"> </span><span class="nf">featureNormalize</span><span class="p">(</span>X<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">mu</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class='line'>    <span class="n">sigma</span> <span class="p">=</span> <span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class='line'>    <span class="n">X_norm</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">./</span> <span class="n">sigma</span><span class="p">;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Step 2. Minimize the cost function</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \|X\theta - y\|^2 = \frac{1}{2m}(X\theta - y)^T (X\theta - y)</script>

<p>where $\theta = (\theta_0, …, \theta_n)^T$</p>

<p>Step 3. Compute hypothesis function as</p>

<script type="math/tex; mode=display">h_\theta(x) = x \cdot \theta = x^T \theta</script>

<p>where vector $x$ is normalized using the same values of $\mu$ and $\sigma$ as in Step 1.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient descent</em> is the method for finding (global) minimum of cost funtion $J(\theta)$.
There are few ways to implement this method.</p>

<h3 id="direct-method">Direct method</h3>

<p>Choose small learning rate $\alpha &gt; 0$ and find the fixed point of the function</p>

<script type="math/tex; mode=display">f(\theta) = \theta - \frac{\alpha}{m}X^T (X\theta - y)</script>

<p><figure class='code'><figcaption><span>gradientDescent.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescent</span><span class="p">(</span>X, y, theta, alpha, num_iters<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">iter</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_iters</span>
</span><span class='line'>        <span class="n">h</span> <span class="p">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</span><span class='line'>        <span class="n">delta</span> <span class="p">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</span><span class='line'>        <span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">X</span>’ <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3 id="optimized-method">Optimized method</h3>

<p>Many mathematical software packages already include implementations of gradient
descent that compute learning rate $\alpha$ automatically.
These methods accept cost function $J(\theta)$ and its gradient
$\nabla J(\theta)$ as arguments, which for the linear regression is computed as follows</p>

<script type="math/tex; mode=display">\nabla J(\theta) = \frac{1}{m}X^T (X\theta - y)</script>

<p><figure class='code'><figcaption><span>costFunction.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunction</span><span class="p">(</span>theta, X, y<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</span><span class='line'>    <span class="n">h</span> <span class="p">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</span><span class='line'>    <span class="n">delta</span> <span class="p">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</span><span class='line'>    <span class="n">J</span> <span class="p">=</span> <span class="n">delta</span>’ <span class="o">*</span> <span class="n">delta</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class='line'>    <span class="n">grad</span> <span class="p">=</span> <span class="n">X</span>’ <span class="o">*</span> <span class="n">delta</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><figcaption><span>gradientDescent.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescent</span><span class="p">(</span>X, y, initial_theta<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span>‘<span class="n">GradObj</span>’<span class="p">,</span> ‘<span class="n">on</span>’<span class="p">,</span> ‘<span class="n">MaxIter</span>’<span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
</span><span class='line'>    <span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">]</span> <span class="p">=</span> <span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunction</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<h2 id="normal-equation">Normal Equation</h2>

<p>Unlike Gradient Descent this method does not require feature normalization (Step 1) and convergence loop. <em>Normal equation</em> gives the closed-form solution to linear regression</p>

<script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y</script>

<p><figure class='code'><figcaption><span>normalEquation.m </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">normalEquation</span><span class="p">(</span>X, y<span class="p">)</span><span class="w"></span>
</span><span class='line'><span class="w">    </span><span class="n">theta</span> <span class="p">=</span> <span class="n">pinv</span><span class="p">(</span><span class="n">X</span>’ <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span>’ <span class="o">*</span> <span class="n">y</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<h2 id="regularization">Regularization</h2>

<p>In case of overfitting both methods can be tweaked by introducing polynomial
features and adjusting equations as follows. Let $\lambda &gt; 0$ and E be the
diagonal matrix</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
E =
\begin{pmatrix}
0 & & & & \\
& 1 & & & \\
& & 1 & & \\
& & & \ddots & \\
& & & & 1 \\
\end{pmatrix}_{n+1 \times n+1} %]]&gt;</script>

<p>Then the cost function for gradient descent becomes</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \left( \|h_\theta(X) - y\|^2 + \lambda \| E\theta \|^2 \right) \\
\nabla J(\theta) = \frac{1}{m} \left( X^T (h_\theta(X) - y) + \lambda E\theta \right)</script>

<p>and normal equation</p>

<script type="math/tex; mode=display">\theta = \left( X^T X +\lambda E \right)^{-1} X^T y</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Another Notation as a Tool of Thought]]></title>
    <link href="http://blog.ndpar.com/2015/04/03/feynman-tot/"/>
    <updated>2015-04-03T08:00:00-04:00</updated>
    <id>http://blog.ndpar.com/2015/04/03/feynman-tot</id>
    <content type="html"><![CDATA[<p>In his seminal <a href="http://www.jsoftware.com/papers/tot.htm">paper</a> Kenneth Iverson described a new mathematical notation which soon became A Programming Language.</p>

<p>Recently, while reading <em>Surely You’re Joking, Mr. Feynman!</em>, I found that Feynman invented his own notation when he was in school.</p>

<!-- more -->

<blockquote>
  <p>While I was doing all this trigonometry, I didn’t like the symbols for sine, cosine, tangent, and so on. To me, $\sin f$ looked like $s$ times $i$ times $n$ times $f$! So I invented another symbol, like a square root sign, that was $\sigma$ with a long arm sticking out of it, and I put the $f$ underneath. For the tangent it was $\tau$ with the top of the tau extended, and for the cosine I made a kind of $\gamma$, but it looked a little bit like the square root sign.</p>
</blockquote>

<blockquote>
  <p>Now the inverse sine was the same sigma, but left-to-right reflected so that it started with the horizontal line with the value underneath, and then the sigma. <em>That</em> was the inverse sine, NOT $\sin^{-1} f$–that was crazy! They had that in books! To me, $\sin^{-1}$ meant $1/\sin$, the reciprocal. So my symbols were better.</p>
</blockquote>

<blockquote>
  <p>I didn’t like $f(x)$–that looked to me like $f$ times $x$. I also didn’t like $dy/dx$–you have a tendency to cancel the d’s–so I made a different sign, something like an &amp; sign. For logarithms it was a big L extended to the right, with the thing you take the log of inside, and so on.</p>
</blockquote>

<blockquote>
  <p>I thought my symbols were just as good, if not better, than the regular symbols–it doesn’t make any difference <em>what</em> symbols you use–but I discovered later that it <em>does</em> make a difference. Once when I was explaining something to another kid in high school, without thinking I started to make these symbols, and he said, “What the hell are those?” I realized then that if I’m going to talk to anybody else, I’ll have to use the standard symbols, so I eventually gave up my own symbols.</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Two series]]></title>
    <link href="http://blog.ndpar.com/2014/11/29/two-series/"/>
    <updated>2014-11-29T07:00:00-05:00</updated>
    <id>http://blog.ndpar.com/2014/11/29/two-series</id>
    <content type="html"><![CDATA[<p>Cliff Pickover <a href="https://twitter.com/pickover/status/538754166261555201">twitted</a> a fun puzzle: Which series is bigger?</p>

<script type="math/tex; mode=display">\sum_{n=0}^\infty \frac{1}{2^{n+1}} \quad \text{or} \quad \sum_{n=0}^\infty \frac{n}{2^{n+1}}</script>

<p>The first one is the famous geometric series which sum is equal to 1. The second one seems to be bigger because 1 &lt; n, except for the 0th term, but that 0th term makes a big difference.</p>

<!-- more -->

<p>First of all, the second series converges. You can prove it by D’Alembert’s theorem</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\frac{a_{n+1}}{a_n} = \frac{n+1}{2^{n+2}}\cdot\frac{2^{n+1}}{n} = \frac{1}{2}\left(1 + \frac{1}{n}\right) \le \frac{3}{4} < 1, \text{for} \; n \ge 2 %]]&gt;</script>

<p>Because the terms are positive, the series converges absolutely, therefore we can rearrange the terms.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align*}
S & = \sum_{n=0}^\infty \frac{n}{2^{n+1}} = \sum_{n=0}^\infty \left(\frac{1}{2^{n+1}} + \frac{n-1}{2^{n+1}}\right) = \sum_{n=0}^\infty \frac{1}{2^{n+1}} + \sum_{n=0}^\infty \frac{n-1}{2^{n+1}} \\
  & = 1 + \frac{1}{2} \sum_{n=0}^\infty \frac{n-1}{2^n} = 1 + \frac{1}{2} \sum_{n=-1}^\infty \frac{n}{2^{n+1}} = 1 - \frac{1}{2} + \frac{1}{2} \sum_{n=0}^\infty \frac{n}{2^{n+1}} \\
  & = \frac{1}{2} + \frac{1}{2}S \implies S = 1
\end{align*} %]]&gt;</script>

<p>Both series are equal.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fast Fourier transform in J]]></title>
    <link href="http://blog.ndpar.com/2014/10/12/fft-j/"/>
    <updated>2014-10-12T08:00:00-04:00</updated>
    <id>http://blog.ndpar.com/2014/10/12/fft-j</id>
    <content type="html"><![CDATA[<p>In the previous <a href="/2014/10/11/dft-j/">post</a> we discussed the discrete Fourier transform (DFT). Its J implementation was pretty straightforward, and the program looked almost identical to its mathematical definition. In this post, I want to explore how to implement fast Fourier transform (FFT), a recursively defined version of DFT that reduces algorithmic complexity from $O(n^2)$ to $O(N\log{N})$.</p>

<h2 id="math">Math</h2>

<p>Recall the DFT formula from the previous post</p>

<script type="math/tex; mode=display">\mathcal{F}_N\mathbf{f}[m] = \sum_{n=0}^{N-1} \mathbf{f}[n] e^{-2\pi i mn/N}, \quad m = 0, \dots , N-1</script>

<p>I use a slightly different notation here to make recursive definition easier to understand.</p>

<p>We assume that <em>N</em> is a power of 2. Using basic arithmetic and properties of complex exponentials, we can derive the formula for the DFT of order <em>2N</em> in terms of two DFTs of order <em>N</em></p>

<script type="math/tex; mode=display">\mathcal{F}_{2N}\mathbf{f}[m]=\mathcal{F}_N\mathbf{f}_{even}+e^{-\pi im/N}\mathcal{F}_N\mathbf{f}_{odd}</script>

<script type="math/tex; mode=display">\mathcal{F}_{2N}\mathbf{f}[m+N]=\mathcal{F}_N\mathbf{f}_{even}-e^{-\pi im/N}\mathcal{F}_N\mathbf{f}_{odd}</script>

<p>where $m=0,\dots,N-1$. The base case, when $N=1$, is</p>

<script type="math/tex; mode=display">\mathcal{F}_1\mathbf{f}[0] = \mathbf{f}[0]</script>

<p>i.e. the identity function.</p>

<!-- more -->

<h2 id="programming">Programming</h2>

<p>Let’s express the FFT algorithm in words. If $N=1$ then the <code>fft</code> function is just the identity function (<code>]</code> in J). Overwise, it’s a function involving a recursive call on an input vector of half the length. Here is the same phrase written in J</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fft=: something_recursive_here ` ] @. (1 = #)</span></code></pre></td></tr></table></div></figure></p>

<p>If $N&gt;1$, we split the input vector into two vectors by even and odd indices. We apply FFT recursively to each of these vectors. We multiply the first output vector by 1 and the second one by $e^{-\pi im/N}$. Then, we find the sum and difference of the result vectors, and concatenate them. That should be it.</p>

<p>Let’s start with the split function. To split a vector by even and odd indices in J, we can split it by groups of two elements, stack the groups on top of each other making an $N\times2$ matrix, then transpose it to get $2\times N$ matrix, which gives us the two vectors we need. Here is the implementation of the split function</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spl=: |:@(_2 ]\ ])</span></code></pre></td></tr></table></div></figure></p>

<p>and here is an example of applying this function to a range vector</p>

<pre><code>   spl 0 1 2 3 4 5 6 7
0 2 4 6
1 3 5 7
</code></pre>

<p>The next step is the function $e^{-\pi im/N}$. We implement it in two steps: first, scalar $\omega=e^{\pi i/N}$, then vector $\boldsymbol\omega=\omega^{-m}$. The implementation is analogous to the one from the previous <a href="/2014/10/11/dft-j/">post</a>, with the difference of $\pi i/N$ instead of $2\pi i/N$, and vector $\boldsymbol\omega$ instead of matrix $\Omega$</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>omg=: ^@o.@(0j1&amp;%)
</span><span class='line'>omv=: omg ^ -@i.</span></code></pre></td></tr></table></div></figure></p>

<p>The new matrix $\Omega$ is build by stacking a unit vector $[1,\dots,1]$ of length <em>N</em> on top of the vector $\boldsymbol\omega$</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Omg=: 1 ,: omv</span></code></pre></td></tr></table></div></figure></p>

<p>Here is an example of its application to $N=4$</p>

<pre><code>   Omg 4
1                  1              1                   1
1 0.707107j_0.707107 2.22045e_16j_1 _0.707107j_0.707107
</code></pre>

<p>Now we can go back to the <code>fft</code> function and add some code to the <code>something_recursive_here</code> placeholder</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fft=: final_piece_here@(Omg@#@{. * $:”1)@spl ` ] @. (1 = #)</span></code></pre></td></tr></table></div></figure></p>

<p><code>$:"1</code> means the <code>fft</code> function is recursively applied to every row of its input matrix (which is the result of the <code>spl</code> function above). <code>Omg@#@{.</code> means: take the first row of the input matrix, get the length of it, and pass it to the <code>Omg</code> function.</p>

<p><code>final_piece_here</code> is then what we described above as “find the sum and difference of the result vectors, and concatenate them”. In J it’s implemented as the fork <code>+/ , -/</code>. Now we have all the pieces in place, and the final program is</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spl=: |:@(_2 ]\ ])
</span><span class='line'>omg=: ^@o.@(0j1&amp;%)
</span><span class='line'>omv=: omg ^ -@i.
</span><span class='line'>Omg=: 1 ,: omv
</span><span class='line'>fft=: (+/ , -/)@(Omg@#@{. * $:”1)@spl ` ] @. (1 = #)</span></code></pre></td></tr></table></div></figure></p>

<h2 id="references">References</h2>

<ul>
  <li>Source code with examples on <a href="https://github.com/ndpar/j/blob/master/fourier.ijs">GitHub</a></li>
  <li>Official FFT add-on on <a href="http://www.jsoftware.com/jwiki/Addons/math/fftw">jsoftware</a> wiki.</li>
</ul>

]]></content>
  </entry>
  
</feed>
